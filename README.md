# Not Just Whatâ€™s There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning

To comply with the double-blind review policy, this GitHub repository is temporarily private while our paper is under review for **AAAI 2026**. It will be made publicly available after the review process is completed. ðŸ±â€ðŸ‘¤

*It will be made publicly accessible after the review process is completed.*ðŸ˜Š

![image-20250804202149663](md_images/README/image-20250804202149663.png)

## ðŸ‘€ CLIPGlasses

Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching "no dog" with dog images). Existing methods refine negation understanding via fine-tuning CLIPâ€™s text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIPâ€™s ability to comprehend negated visual descriptions. CLIPGlasses adapts a dual-stage design: a Lens module disentangles negated semantics from text embeddings, and a Frame module predicts context-aware repulsion strength, which is integrated into the modified similarity computation to penalize alignment with negated semantics, thereby reducing false positive matches. Experiments show that CLIP equipped with CLIPGlasses achieves competitive in-domain performance and outperforms state-of-the-art methods in cross-domain generalization. Its superiority is especially evident under low-resource conditions, indicating stronger robustness across domains. Source code is included in the supplementary material.

our key contributions as follows:

* We present **CLIPGlasses**, a non-intrusive framework enhancing CLIP's negation modeling via human-inspired two-stage processing without parameter modification.
* We design a novel architecture, including a syntax-semantic **Lens** for disentangling negation semantics, and a **Frame** for modeling context-aware repulsion, and a modified similarity computation that explicitly reverses alignment with negated content.
* Our method attains **state-of-the-art trade-offs between in-domain accuracy and cross-domain generalization**, without compromising CLIPâ€™s native zero-shot abilities.

## ðŸ›  How to Use

### 1. Environment Set Up

First, install all required dependencies:

```shell
pip install -r requirements.txt
```

### 2. Download Datasets

This project relies on several benchmark datasets for model training and evaluation.
Please download them following the corresponding papers or dataset websites.

* CCNeg

  > Singh, Jaisidh et al., *Learning the Power of â€œNoâ€: Foundation Models with Negations*, WACV 2025.

* NegBench

  > Alhamoud, Kumail et al., *Vision-language models do not understand negation*, arXiv 2025.

* ImageNet

  > Deng, Jia et al., *ImageNet: A Large-Scale Hierarchical Image Database*, CVPR 2009.

* Caltech

  > Li, Fei-Fei et al., *Caltech 101*, CaltechDATA 2022.

* COCO

  > Lin, Tsung-Yi et al., *Microsoft COCO: Common Objects in Context*, arXiv 2015.

### 3. Run Train or Test

Navigate to the experiment folder and run:

```shell
cd Glasses\exp\exp5_glasses
python3 Glasses.py
```



# CLIPGlasses
